# app.py
import gradio as gr
import os
import json
from datetime import datetime
from src.document_processor import load_multiple_documents, split_documents
from src.vector_store import create_vectorstore, save_vectorstore, load_vectorstore
from src.chat_chain import create_rag_chain, format_sources
from src.llm_handler import get_llm, get_available_models
from src.export_handler import export_chat_to_pdf, export_chat_to_json  # <-- Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½ Ð¸Ð¼Ð¿Ð¾Ñ€Ñ‚
from config.settings import settings
import logging

# ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ
vectorstore = None
qa_chain = None
current_model = None
chat_history = []  # Ð”Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°

def process_documents(files):
    """ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²"""
    global vectorstore
    try:
        if not files:
            return "âŒ ÐÐµ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ñ‹ Ñ„Ð°Ð¹Ð»Ñ‹ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸!"
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿ÑƒÑ‚Ð¸ Ðº Ñ„Ð°Ð¹Ð»Ð°Ð¼
        file_paths = [f.name for f in files]
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð²ÑÐµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹
        documents = load_multiple_documents(file_paths)
        
        if not documents:
            return "âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð½Ð¸ Ð¾Ð´Ð¸Ð½ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚!"
        
        # Ð Ð°Ð·Ð´ÐµÐ»ÑÐµÐ¼ Ð½Ð° Ñ‡Ð°Ð½ÐºÐ¸
        texts = split_documents(documents)
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ðµ Ñ…Ñ€Ð°Ð½Ð¸Ð»Ð¸Ñ‰Ðµ
        vectorstore = create_vectorstore(texts)
        save_vectorstore(vectorstore)
        
        return f"âœ… ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð¾ {len(files)} Ñ„Ð°Ð¹Ð»Ð¾Ð². Ð’ÑÐµÐ³Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {len(documents)}, Ñ‡Ð°Ð½ÐºÐ¾Ð²: {len(texts)}"
    except Exception as e:
        error_msg = f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {str(e)}"
        logger.error(error_msg)
        return error_msg

def initialize_chat(model_name_key):
    """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ñ‡Ð°Ñ‚-Ð±Ð¾Ñ‚Ð° Ñ Ð²Ñ‹Ð±Ñ€Ð°Ð½Ð½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ"""
    global vectorstore, qa_chain, current_model
    try:
        if vectorstore is None:
            return "", "Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð¹Ñ‚Ðµ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹!", ""
        
        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿Ð¾Ð»Ð½Ð¾Ðµ Ð¸Ð¼Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        available_models = get_available_models()
        model_name = available_models.get(model_name_key, settings.DEFAULT_MODEL)
        current_model = model_name_key
        
        llm = get_llm(model_name)
        qa_chain = create_rag_chain(vectorstore, llm)
        message = f"âœ… Ð§Ð°Ñ‚-Ð±Ð¾Ñ‚ Ð³Ð¾Ñ‚Ð¾Ð² Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ðµ! Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ {model_name_key}"
        return "", message, ""
    except Exception as e:
        error_msg = f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {str(e)}"
        logger.error(error_msg)
        return "", error_msg, ""

def chat(message, history):
    """Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ñ‡Ð°Ñ‚Ð° Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²"""
    global qa_chain, chat_history
    if qa_chain is None:
        return "", history, "Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ñ‡Ð°Ñ‚-Ð±Ð¾Ñ‚Ð°!"
    
    try:
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ Ð´Ð¸Ð°Ð»Ð¾Ð³
        chat_history.append((message, ""))  # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ñ‡Ð°Ñ‚-Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð´Ð»Ñ RAG Ñ†ÐµÐ¿Ð¾Ñ‡ÐºÐ¸
        chat_history_formatted = [(human, ai) for human, ai in chat_history]
        
        result = qa_chain({"question": message, "chat_history": chat_history_formatted})
        answer = result["answer"]
        
        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð¼ Ð±Ð¾Ñ‚Ð°
        chat_history[-1] = (chat_history[-1][0], answer)  # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ð¹ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚
        
        # Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸
        sources = format_sources(result["source_documents"])
        sources_text = "ðŸ“š **Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸:**\n\n"
        for i, source in enumerate(sources[:3], 1):  # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ðµ 3 Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°
            sources_text += f"**Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº {i}:**\n"
            sources_text += f"{source['content']}\n"
            if source['metadata']:
                sources_text += f"*ÐœÐµÑ‚Ð°Ð´Ð°Ð½Ð½Ñ‹Ðµ: {source['metadata']}*\n\n"
        
        return "", history + [(message, answer)], sources_text
    except Exception as e:
        error_msg = f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {str(e)}"
        return "", history, error_msg

def clear_chat():
    """ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ñ‡Ð°Ñ‚Ð°"""
    global chat_history
    chat_history = []
    return []

# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ð¸ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡ÐµÐ¹ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
def export_chat_json_wrapper():
    """ÐžÐ±ÐµÑ€Ñ‚ÐºÐ° Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° Ñ‡Ð°Ñ‚Ð° Ð² JSON"""
    global chat_history, current_model
    try:
        result = export_chat_to_json(chat_history, current_model)
        return result
    except Exception as e:
        error_msg = f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°: {str(e)}"
        logger.error(error_msg)
        return error_msg

def export_chat_pdf_wrapper():
    """ÐžÐ±ÐµÑ€Ñ‚ÐºÐ° Ð´Ð»Ñ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð° Ñ‡Ð°Ñ‚Ð° Ð² PDF"""
    global chat_history, current_model
    try:
        result = export_chat_to_pdf(chat_history, current_model)
        return result
    except Exception as e:
        error_msg = f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°: {str(e)}"
        logger.error(error_msg)
        return error_msg

# Ð˜Ð½Ñ‚ÐµÑ€Ñ„ÐµÐ¹Ñ Gradio
with gr.Blocks(title="RAG Chatbot Advanced") as demo:
    gr.Markdown("# ðŸ¤– RAG Chatbot Ñ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑÐ¼Ð¸")
    gr.Markdown("ÐŸÑ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‡Ð°Ñ‚-Ð±Ð¾Ñ‚ Ñ Retrieval-Augmented Generation")
    
    with gr.Tab("Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹"):
        with gr.Row():
            with gr.Column():
                file_input = gr.File(
                    label="Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ (PDF/TXT/DOCX/HTML/MD)", 
                    file_count="multiple"
                )
                process_btn = gr.Button("ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹")
                status1 = gr.Textbox(label="Ð¡Ñ‚Ð°Ñ‚ÑƒÑ")
                process_btn.click(process_documents, inputs=file_input, outputs=status1)
            
            with gr.Column():
                model_dropdown = gr.Dropdown(
                    choices=list(get_available_models().keys()),
                    value="Claude Sonnet 4",
                    label="Ð’Ñ‹Ð±ÐµÑ€Ð¸Ñ‚Ðµ Ð¼Ð¾Ð´ÐµÐ»ÑŒ LLM"
                )
                init_btn = gr.Button("Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ‡Ð°Ñ‚-Ð±Ð¾Ñ‚Ð°")
                status2 = gr.Textbox(label="Ð¡Ñ‚Ð°Ñ‚ÑƒÑ")
                init_btn.click(initialize_chat, inputs=model_dropdown, outputs=[model_dropdown, status2, status1])
    
    with gr.Tab("Ð§Ð°Ñ‚"):
        with gr.Row():
            with gr.Column(scale=2):
                chatbot = gr.Chatbot(label="Ð”Ð¸Ð°Ð»Ð¾Ð³", height=500)
                with gr.Row():
                    msg = gr.Textbox(
                        label="Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð²Ð°Ñˆ Ð²Ð¾Ð¿Ñ€Ð¾Ñ", 
                        placeholder="Ð—Ð°Ð´Ð°Ð¹Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ð¾ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼...",
                        scale=8
                    )
                    clear_btn = gr.Button("ÐžÑ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ", scale=1)
                
                # ÐšÐ½Ð¾Ð¿ÐºÐ¸ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°
                with gr.Row():
                    export_json_btn = gr.Button("Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð² JSON")
                    export_pdf_btn = gr.Button("Ð­ÐºÑÐ¿Ð¾Ñ€Ñ‚ Ð² PDF")
                    export_status = gr.Textbox(label="Ð¡Ñ‚Ð°Ñ‚ÑƒÑ ÑÐºÑÐ¿Ð¾Ñ€Ñ‚Ð°")
                
            with gr.Column(scale=1):
                sources_output = gr.Markdown(label="Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸", height=500)
        
        msg.submit(chat, [msg, chatbot], [msg, chatbot, sources_output])
        clear_btn.click(clear_chat, None, chatbot)
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ð±ÐµÑ€Ñ‚ÐºÐ¸ Ð´Ð»Ñ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ‡Ð¸ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
        export_json_btn.click(export_chat_json_wrapper, outputs=export_status)
        export_pdf_btn.click(export_chat_pdf_wrapper, outputs=export_status)

if __name__ == "__main__":
    demo.launch()